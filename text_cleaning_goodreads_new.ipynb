{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob, Word\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tu\n",
      "[nltk_data]     Lam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Tu\n",
      "[nltk_data]     Lam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Tu\n",
      "[nltk_data]     Lam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Tu Lam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "STOPWORDS = stopwords.words(\"english\") #stopwords are the most common unnecessary words. eg is, he, that, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii') # A function to remove emojis from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_postag(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contraction convert, spelling check\n",
    "spell = Speller(lang='en')\n",
    "contractions_dict = {     \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I had\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"iit will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove number\n",
    "def more_clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    output = ''.join(c for c in text if not c.isdigit())\n",
    "    text=expand_contractions(output,contractions_dict)\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    spells = ' '.join(spell(w) for w in (word_tokens))\n",
    "    return spells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove 'not' for sentiment analysis\n",
    "STOPWORDS.remove('not')\n",
    "len(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text=deEmojify(text) # remove emojis\n",
    "    text_cleaned=\"\".join([x for x in text if x not in string.punctuation]) # remove punctuation\n",
    "    text_cleaned=re.sub(' +', ' ', text_cleaned) # remove extra white spaces\n",
    "    text_cleaned=text_cleaned.lower() # converting to lowercase\n",
    "    text_cleaned=more_clean_text(text_cleaned) # remove numbers\n",
    "    text_cleaned = expand_contractions(text_cleaned, contractions_dict) # contraction & spelling check\n",
    "    \n",
    "    tokens=text_cleaned.split(\" \")\n",
    "    tokens=[token for token in tokens if token not in STOPWORDS] # Taking only those words which are not stopwords\n",
    "    \n",
    "    #Converting to lemma\n",
    "    text_cleaned = lemmatize_with_postag(str(tokens))\n",
    "    for r in ((\"\\' \", ''), ('\\'', ''), ('[',''),  (']','')):\n",
    "        text_cleaned = text_cleaned.replace(*r)\n",
    "    return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>22551730</td>\n",
       "      <td>4</td>\n",
       "      <td>Dec 14, 2016</td>\n",
       "      <td>0307408868</td>\n",
       "      <td>Another hard to put down nonfiction book from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18176747</td>\n",
       "      <td>5</td>\n",
       "      <td>Dec 21, 2016</td>\n",
       "      <td>0062273205</td>\n",
       "      <td>I haven't read many (any?) books that are writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>137554</td>\n",
       "      <td>0</td>\n",
       "      <td>Mar 20, 2014</td>\n",
       "      <td>006073731X</td>\n",
       "      <td>Sacca and Nate recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>40955</td>\n",
       "      <td>5</td>\n",
       "      <td>Dec 21, 2016</td>\n",
       "      <td>0071424911</td>\n",
       "      <td>A truly inspirational book by a truly inspirat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9850443</td>\n",
       "      <td>3</td>\n",
       "      <td>Aug 05, 2012</td>\n",
       "      <td>0062041266</td>\n",
       "      <td>A fun, dark, slightly comical western about tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906871</td>\n",
       "      <td>4405141</td>\n",
       "      <td>3</td>\n",
       "      <td>Aug 19, 2014</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>While i liked it and appreciated all the infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906872</td>\n",
       "      <td>4405141</td>\n",
       "      <td>5</td>\n",
       "      <td>Apr 15, 2013</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>If you know anyone suffering from an eating di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906873</td>\n",
       "      <td>4405141</td>\n",
       "      <td>5</td>\n",
       "      <td>Jul 28, 2015</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>Fabulous insight to what people struggling wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906874</td>\n",
       "      <td>4405141</td>\n",
       "      <td>5</td>\n",
       "      <td>Mar 30, 2009</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>This is an excellent resource -best book I hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906875</td>\n",
       "      <td>4405141</td>\n",
       "      <td>4</td>\n",
       "      <td>Aug 09, 2015</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>Good reminders of things I already knee, but h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>906876 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         book_id  overall    reviewTime        asin  \\\n",
       "0       22551730        4  Dec 14, 2016  0307408868   \n",
       "1       18176747        5  Dec 21, 2016  0062273205   \n",
       "2         137554        0  Mar 20, 2014  006073731X   \n",
       "3          40955        5  Dec 21, 2016  0071424911   \n",
       "4        9850443        3  Aug 05, 2012  0062041266   \n",
       "...          ...      ...           ...         ...   \n",
       "906871   4405141        3  Aug 19, 2014  0061698954   \n",
       "906872   4405141        5  Apr 15, 2013  0061698954   \n",
       "906873   4405141        5  Jul 28, 2015  0061698954   \n",
       "906874   4405141        5  Mar 30, 2009  0061698954   \n",
       "906875   4405141        4  Aug 09, 2015  0061698954   \n",
       "\n",
       "                                               reviewText  \n",
       "0       Another hard to put down nonfiction book from ...  \n",
       "1       I haven't read many (any?) books that are writ...  \n",
       "2                                Sacca and Nate recommend  \n",
       "3       A truly inspirational book by a truly inspirat...  \n",
       "4       A fun, dark, slightly comical western about tw...  \n",
       "...                                                   ...  \n",
       "906871  While i liked it and appreciated all the infor...  \n",
       "906872  If you know anyone suffering from an eating di...  \n",
       "906873  Fabulous insight to what people struggling wit...  \n",
       "906874  This is an excellent resource -best book I hav...  \n",
       "906875  Good reminders of things I already knee, but h...  \n",
       "\n",
       "[906876 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load current dataframe\n",
    "df = pd.read_csv('final_goodreads.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>22551730</td>\n",
       "      <td>4</td>\n",
       "      <td>Dec 14, 2016</td>\n",
       "      <td>0307408868</td>\n",
       "      <td>Another hard to put down nonfiction book from ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18176747</td>\n",
       "      <td>5</td>\n",
       "      <td>Dec 21, 2016</td>\n",
       "      <td>0062273205</td>\n",
       "      <td>I haven't read many (any?) books that are writ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>137554</td>\n",
       "      <td>0</td>\n",
       "      <td>Mar 20, 2014</td>\n",
       "      <td>006073731X</td>\n",
       "      <td>Sacca and Nate recommend</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>40955</td>\n",
       "      <td>5</td>\n",
       "      <td>Dec 21, 2016</td>\n",
       "      <td>0071424911</td>\n",
       "      <td>A truly inspirational book by a truly inspirat...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9850443</td>\n",
       "      <td>3</td>\n",
       "      <td>Aug 05, 2012</td>\n",
       "      <td>0062041266</td>\n",
       "      <td>A fun, dark, slightly comical western about tw...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906871</td>\n",
       "      <td>4405141</td>\n",
       "      <td>3</td>\n",
       "      <td>Aug 19, 2014</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>While i liked it and appreciated all the infor...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906872</td>\n",
       "      <td>4405141</td>\n",
       "      <td>5</td>\n",
       "      <td>Apr 15, 2013</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>If you know anyone suffering from an eating di...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906873</td>\n",
       "      <td>4405141</td>\n",
       "      <td>5</td>\n",
       "      <td>Jul 28, 2015</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>Fabulous insight to what people struggling wit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906874</td>\n",
       "      <td>4405141</td>\n",
       "      <td>5</td>\n",
       "      <td>Mar 30, 2009</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>This is an excellent resource -best book I hav...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906875</td>\n",
       "      <td>4405141</td>\n",
       "      <td>4</td>\n",
       "      <td>Aug 09, 2015</td>\n",
       "      <td>0061698954</td>\n",
       "      <td>Good reminders of things I already knee, but h...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>906876 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         book_id  overall    reviewTime        asin  \\\n",
       "0       22551730        4  Dec 14, 2016  0307408868   \n",
       "1       18176747        5  Dec 21, 2016  0062273205   \n",
       "2         137554        0  Mar 20, 2014  006073731X   \n",
       "3          40955        5  Dec 21, 2016  0071424911   \n",
       "4        9850443        3  Aug 05, 2012  0062041266   \n",
       "...          ...      ...           ...         ...   \n",
       "906871   4405141        3  Aug 19, 2014  0061698954   \n",
       "906872   4405141        5  Apr 15, 2013  0061698954   \n",
       "906873   4405141        5  Jul 28, 2015  0061698954   \n",
       "906874   4405141        5  Mar 30, 2009  0061698954   \n",
       "906875   4405141        4  Aug 09, 2015  0061698954   \n",
       "\n",
       "                                               reviewText  cleaned_text  \n",
       "0       Another hard to put down nonfiction book from ...           NaN  \n",
       "1       I haven't read many (any?) books that are writ...           NaN  \n",
       "2                                Sacca and Nate recommend           NaN  \n",
       "3       A truly inspirational book by a truly inspirat...           NaN  \n",
       "4       A fun, dark, slightly comical western about tw...           NaN  \n",
       "...                                                   ...           ...  \n",
       "906871  While i liked it and appreciated all the infor...           NaN  \n",
       "906872  If you know anyone suffering from an eating di...           NaN  \n",
       "906873  Fabulous insight to what people struggling wit...           NaN  \n",
       "906874  This is an excellent resource -best book I hav...           NaN  \n",
       "906875  Good reminders of things I already knee, but h...           NaN  \n",
       "\n",
       "[906876 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new column for cleaned text\n",
    "df['cleaned_text'] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tu Lam\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# Track progress\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████▌                                                   | 220641/906876 [12:47:24<20:22:49,  9.35it/s]"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "df['cleaned_text'] = df.progress_apply(lambda row: clean_text(row.reviewText), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to csv\n",
    "df.to_csv('goodreads_processed_text_new.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
